---
title: "Gozde and Liora's Replication Code"
author: "Gozde Guran and Liora Goldensher"
date: "3/6/2015"
output: html_document
---

# Replication Code for "Contextual Factors and the Extreme Right Vote in Europe, 1980-2002" by Kai Arzheimer American Journal of Political Science (2009)

Note: Our regression values are slightly different from the authors as his stata code used a slightly different method than our function (glmmPQL) to calculate Penalized Quasi-likehood. Finally, we were unfortunately not able to reproduce the last two figures that showed the joint impact of more than 2 variables.  These graphs are dependant on a concept called "covariate contributions" developed by Mitchell and Chen (2005), that represents the aggregation into a single quantity of individual effects of all independant variables that are not varied in a graph.  Graphs based on this concept were beyond the scope of our understanding or ability for this project.  


#The paper
Kai Arzheimer, "Contextual Factors and the Extreme Right Vote in Western Europe, 1980-2002"25548117.pdf
This article attempts to answer the “twin question of why the extreme right support is so unstable within many countries over time, and why these parties are so weak in many West European countries.” The author conduct a multi-level analysis, combining data on macro-level contextual variables (unemployment, immigration, and welfare benefits) with individual-level variables (sociodemographic factors and attitudes). It concludes that although the contextual factors are positively correlated with the intention to vote for an extreme-right party, they don't seem to reinforce each other (i.e. as unemployment levels rise, the effect of immigration does not increase but rather declines).

#The data
Individual-level variables come from Eurobarometer surveys (1980-2002), while contextual variables are gathered from a number of different data sets produced by the OECD, UNHCR etc. The merged data and the Stata code are available through the author’s dataverse at: http://hdl.handle.net/1902.1/12092. 

#The model 
The article uses a multi-level logistic model via Penalized Quasi-Likelihood. The dependent variable is vote intention for an extreme right party, which is assumed to be binomially distributed.

# Run necessary packages and load data (Source: Arzheimer dataverse)

```{r}


library(MASS) # includes the glmmPQL function to run model
library(nlme) # for non-linear mixed effects models
library(foreign) # allows us to read in data from another format
library(dplyr) # allows us to manipulate our dataset
library(broom) # helps tidy data and present in a more usable form
library(ggplot2) # for graphing
library(stargazer) # used to create regression tables
library(grid) # used to combine ggplot graphs
library(gridExtra) # used to combine ggplot graphs
library(msm) #includes deltamethod function, which allows us to obtain standard errors of transformed parameters

data <- read.dta("nonimp.dta")

# Cleaning up data
data <- data %>%
  rename(ext_vote = rexvote,
         age_1 = age1, # (18–29 years)
         age_2 = age2, # (30–45 years)
         age_4 = age4, # (>65 years)
         edu_1 = mye1, # Education: middle/high
         edu_2 = mye2, # Education: university
         farmer_own = farmerown,
         left_right_scale = zlrs, # Left-Right Scale
         eu_neg = euschlecht, # Negative evaluation of EU membership of one's own country
         z_dem_satis = zsatisdmo, # Dissatisfied: Democracy
         dispro_elect = disp, # Disproportionality
         federalism = lfed1, # Decentralization
         z_asylumseeker = zasylumseekers,
         z_unemp = zsur, # Unemployment rate
         z_replacement = zreplacementrate, # Unemp benefits
         max_er = rmax) # Maximum toughness in extreme right discourse
```


# Preparing data for analysis
Lines 64-83 of this code are based on (though not identical to) the code written by Harvard students in their previous replication of this paper. Citation : Noam Gidron; Jeffrey Javed, 2011, "Replication data for: The Unholy Trinity: Immigration, Unemployment and Extremism in Europe, 1980-2002", http://hdl.handle.net/1902.1/15902 V1


```{r}
# Create centered variables for Saliance and Variance based on grand mean centering according to author

data <- data  %>% 
  mutate(salience_mean_c = (salienzmean - 3.84568)) %>% 
  mutate(var_c = rvar - 21.75423)

# Make male variable numeric
data <- data %>%
  mutate(male = as.numeric(male))

# Create a dummy variable for country (transform from character to factor variable)
data <- data %>%
  mutate(country = factor(sortcountry, labels = c("AT", "BE","DE-E","DE-W","DK","ES","FI","FR","GR","IT","LU","NL","NO","PT","SE")))
```

# Run the regression (multilevel logictic regression based on Quasi-penalized likelihood): All contextual variables are standardized mean-centered (as connoted by "z-")

```{r}
model <- glmmPQL(ext_vote ~ male + age_1 + age_2 + age_4 + edu_1 + edu_2 + farmer_own + worker + retired + unemployed + left_right_scale + eu_neg + z_dem_satis + dispro_elect + federalism + z_asylumseeker + z_unemp + z_asylumseeker:z_unemp + z_replacement + z_replacement:z_unemp + z_replacement:z_asylumseeker + max_er + salience_mean_c + var_c + var_c:salience_mean_c + country - 1, random = ~1|kontext, family = binomial(link = "logit"), data = data, verbose = TRUE)
# summary(model) # Output too long
```


# Table 1
![Table 1](figures/table1.png)
```{r}
# Create vectors of coefficients, standard errors, and variable names

coefficient <- as.vector(model$coef$fixed) 
standard_error <- as.vector(sqrt(diag(model$varFix))) 

# Create matrix of coefficients and standard deviations.
table_matrix <- cbind(coefficient, standard_error)

# Label matrix.

rownames(table_matrix) <- c("Male","18_29 years","30_45 years",">65 years","Education: middle/high","Education: university","Petty Bourgeoisie","Worker","Pensioner","Unemployed","Left-Right","Dissatisfied: EU","Dissatisfied: Democracy","Disproportionality","Decentralization","Asylumseekers", "Unemployment", "Asylumseekers x Unemployment", "Toughness", "Salience", "Variance", "AT", "BE", "DE-E", "DE-W", "DK", "ES", "FI", "FR", "GR", "IT", "NL", "NO", "PT", "SE", "Asylumseekers x Unemployment", "Unemployment benefits x Unemployment", "Unemployment benefits x Asylumseekers", "Variance x Salience")

# Create Table 1 with stargaze

stargazer(table_matrix, title = "TABLE 1 Support for the Extreme Right: Sociodemographics, Attitudes, Country Effects, and Contextual Variables", align = TRUE, table.layout = "t", type = "text", notes = "Logistic multilevel model. PQL2 estimates and model-based standard errors.", style = "ajps", out = "Table 1.html", no.space = TRUE)
```

# Figure 1 (Conditonal Effects of Unemployment and Immigration)
![Figure 1](figures/fig1.png)
```{r}
#Function with which we will create each graph
graph <- function(data, ratio, ymin, ymax) {
  ggplot(data, aes(x = variable, y = slopes)) + 
  geom_line(aes(y = slopes), color = "blue") + #color will always be blue
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + 
  geom_hline(yintercept = 0, size = 0.4, color = "black") + #fixed y intercept and color
  labs(x = xtitle, y = ytitle, title = main) + #titles will call from prepared data
  coord_fixed(ratio)
}
#Preparing the data for the first graph
asy <- seq(from = min(data$z_asylumseeker), to = max(data$z_asylumseeker), by = .01) # vector with possible values for asylumseekers
variable <- asy # identifying "asy" as the variable of interest so that the remaining code can be reused in the next graph with a different variable of interest specified
slopes <- model$coef$fixed["z_unemp"] + 
  model$coef$fixed["z_asylumseeker:z_unemp"] * asy # Estimated slopes based on estimated coefficients of unemployment (17) and unemployment x immigration (36)
estmean <- model$coef$fixed # Estimated means for each variable 
estvar <- vcov(model) # Estimated covariances
se <- rep(NA, length(variable)) # Creating empty vector for predicted s.e. values
# Creating a loop using the deltamethod function which approximates standard errors of transformation of parameters based on the given estimates of the mean and covariance (~ bootstrapping).
for (i in 1:length(variable)) {
  j <- variable[i]
  se[i] <- deltamethod (~ (x17) + (x36) * j, estmean, estvar)
}

# Create confidence intervals and combine all values in matrix.
upper <- slopes + 1.96 * se
lower <- slopes - 1.96 * se
variable.data <- cbind(variable, slopes, upper, lower)
variable.data <- data.frame(variable.data) # Turn matrix into data frame so that ggplot can read it.
xtitle <- "Asylumseekers"
ytitle <- "Effect: Unemployment"
main <- "Figure 1: The Conditional Effects of Unemployment and Immigration"

#creating the first graph using our ggplot function
graph_1 <- graph(data = variable.data, ratio = 8)
graph_1

#Preparing the data for the second graph
unemployment <- seq(from = min(data$z_unemp), to = max(data$z_unemp), by = .01) # vector with possible values for unemployment
variable <- unemployment # standardized label
slopes <- model$coef$fixed["z_asylumseeker"] + model$coef$fixed["z_asylumseeker:z_unemp"] * variable # Estimated slopes based on estimated coefficients of asylumseekers (16) and unemployment x immigration (36)
estmean <- model$coef$fixed # Estimated means for variables 
estvar <- vcov(model) # Estimated covariances
se <- rep(NA, length(variable)) # Creating empty vector for predicted s.e. values
# Creating a loop with deltamethod to calculate estimated standard errors
for (i in 1:length(variable)) {
  j <- variable[i]
  se[i] <- deltamethod (~ (x16) + (x36) * j, estmean, estvar)
}
# Create confidence intervals and combine all values in matrix.
upper <- slopes + 1.96 * se
lower <- slopes - 1.96 * se
variable.data <- cbind(variable, slopes, upper, lower)
variable.data <- data.frame(variable.data) # Turn matrix into data frame so that ggplot can read it.
xtitle <- "Unemployment"
ytitle <- "Effect: Asylum seekers"
main <- "The Conditional Effects of Unemployment and Immigration"
#Creating the second graph with our function
graph_2 <- graph(data = variable.data, ratio = 12)
graph_2

# Figure 1 with ggplot
grid.arrange(graph_1, graph_2, ncol = 1)

# Plot separately - better scaled
graph_1
graph_2
```

# Figure 2 (Conditonal Effects of Salience and Variance)
![Figure 2](figures/fig2.png)

```{r}
# Figure 2, graph 1
#Preparning the data to graph
salience <- seq(from = min(data$salience_mean_c), to = max(data$salience_mean_c), by=.1) # vector with possible values for salience
variable <- salience
slopes <- model$coef$fixed["var_c"] + model$coef$fixed["salience_mean_c:var_c"] * variable # Estimated slopes based on estimated coefficients of salience (21) and salience x variance (39)
estmean <- model$coef$fixed # Estimated means for variables 
estvar <- vcov(model) # Estimated covariances
se <- rep(NA, length(variable)) # Creating empty vector for predicted s.e. values
# Creating a loop with deltamethod to calculate estimated standard errors
for (i in 1:length(variable)) {
  j <- variable[i]
  se[i] <- deltamethod (~ (x21) + (x39)*j, estmean, estvar)
}
# Create confidence intervals and combine all values in matrix.
upper <- slopes + 1.96*se
lower <- slopes - 1.96*se
variable.data <- cbind(variable, slopes, upper, lower)
variable.data <- data.frame(variable.data) # Turn matrix into data frame so that ggplot can read it.
xtitle <- "Salience"
ytitle <- "Effect: Variance"
main <- "The Conditional Effects of Salience and Variance"

#Creating the first graph with our function
graph_3 <- graph(data = variable.data, ratio = 400)
graph_3

# Second part of Figure 2:
#Preparing the data to graph
variance <- seq(from = min(data$var_c), to = max(data$var_c), by=10) # vector with possible values for variance
variable <- variance
slopes <- model$coef$fixed["salience_mean_c"] + model$coef$fixed["salience_mean_c:var_c"] * variable # Estimated slopes based on estimated coefficients of salience (20) and salience x variance (39)
estmean <- model$coef$fixed # Estimated means for variables 
estvar <- vcov(model) # Estimated covariances
se <- rep(NA, length(variable)) # Creating empty vector for predicted s.e. values
# Creating a loop with deltamethod to calculate estimated standard errors

for (i in 1:length(variable)) {
  j <- variable[i]
  se[i] <- deltamethod (~ (x20) + (x39) * j, estmean, estvar)
}
# Create confidence intervals and combine all values in matrix.
upper <- slopes + 1.96 * se
lower <- slopes - 1.96 * se
variable.data <- cbind(variable, slopes, upper, lower)
variable.data <- data.frame(variable.data) # Turn matrix into data frame so that ggplot can read it.
xtitle <- "Variance"
ytitle <- "Effect: Salience"
main <- "The Conditional Effects of Salience and Variance"

#Creating the second graph with our function
graph_4 <- graph(data = variable.data, ratio = 500)
graph_4

grid.arrange(graph_3, graph_4, ncol = 1)
# Better scaled versions - not combined
graph_3
graph_4
```

